ğŸ¤ Speech Emotion Recognition (SER) Web App

A deep learningâ€“based Speech Emotion Recognition system built using PyTorch and deployed as an interactive Streamlit web application.
The model predicts human emotions from speech audio using MFCC features and a CNN architecture.

ğŸš€ Features

ğŸµ Emotion prediction from .wav audio files

ğŸ™ï¸ Live voice recording and real-time prediction

ğŸ§  CNN-based deep learning model (PyTorch)

âš¡ GPU acceleration supported (CUDA)

ğŸ“Š Confidence score for predictions

ğŸŒ Interactive web interface using Streamlit

ğŸ§  Emotions Supported

The model classifies speech into 8 emotions:

Label	Emotion
0	Neutral
1	Calm
2	Happy
3	Sad
4	Angry
5	Fearful
6	Disgust
7	Surprised
ğŸ—ï¸ Project Structure
speech_emotion/
â”‚
â”œâ”€â”€ app.py                 # Streamlit web app
â”œâ”€â”€ predict.py             # CLI-based prediction
â”œâ”€â”€ predict_utils.py       # Prediction logic
â”‚
â”œâ”€â”€ model.py               # CNN architecture
â”œâ”€â”€ features.py            # MFCC feature extraction
â”‚
â”œâ”€â”€ results/
â”‚   â””â”€â”€ best_emotion_cnn.pth   # Trained model weights
â”‚
â”œâ”€â”€ requirements.txt       # Required libraries
â””â”€â”€ README.md              # Project documentation

âš™ï¸ Installation & Setup
1ï¸âƒ£ Create and activate virtual environment
python -m venv venv
venv\Scripts\activate

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

â–¶ï¸ Run the Web App (Recommended)
streamlit run web_app.py


Then open the browser URL shown in the terminal.

App Capabilities:

Upload a .wav audio file

Record voice using microphone

Get predicted emotion + confidence

â–¶ï¸ Run Prediction via Terminal (Optional)

Place a .wav file in the project folder

Update the file path in predict.py

Run:

python predict.py

ğŸ§ª Model Training (Already Done)

Dataset: RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)

Features: MFCC (Mel-Frequency Cepstral Coefficients)

Architecture: Convolutional Neural Network (CNN)

Train/Validation/Test Split: 70% / 15% / 15%

Early stopping applied to prevent overfitting

Final Performance (Test Set):

Accuracy: ~67%

F1-score: ~0.67

ROCâ€“AUC: ~0.94

ğŸ“Š Evaluation Metrics

The following metrics were used:

Accuracy

Precision

Recall

F1-score

Confusion Matrix

ROCâ€“AUC Curve

(All evaluation results and plots are saved in the results/ folder.)

ğŸ’¡ Technologies Used

Python 3.10

PyTorch

Librosa

NumPy

Scikit-learn

Streamlit

Matplotlib

Sounddevice

ğŸ“Œ Notes

GPU is optional for prediction (CPU works fine).

Dataset files are not included due to size and licensing.

The trained model (.pth) is already provided.

ğŸ“ˆ Future Improvements (Optional)

CNN + LSTM architecture

Mel-spectrogram features

Real-time emotion visualization

Deployment on Streamlit Cloud / HuggingFace Spaces

ğŸ‘¨â€ğŸ’» Author

Speech Emotion Recognition Project
Built as a complete end-to-end ML application using PyTorch and Streamlit.

ğŸ”¥ This project demonstrates a full ML pipeline â€” training, evaluation, inference, and deployment.